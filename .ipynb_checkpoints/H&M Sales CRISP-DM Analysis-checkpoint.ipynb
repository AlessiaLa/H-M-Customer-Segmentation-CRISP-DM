{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import numpy.linalg as ln\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# Importing the Datasets CSV\n",
    "\n",
    "dataset_customers = \"C:/Users/Alessia/Documents/GitHub/H-M-Customer-Segmentation-CRISP-DM/Dataset/customers.csv\"\n",
    "dataset_articles = \"C:/Users/Alessia/Documents/GitHub/H-M-Customer-Segmentation-CRISP-DM/Dataset/articles.csv\"\n",
    "dataset_transactions = \"C:/Users/Alessia/Documents/GitHub/H-M-Customer-Segmentation-CRISP-DM/Dataset/transactions_train.csv\"\n",
    "\n",
    "# Read Datasets\n",
    "customers = pd.read_csv(dataset_customers)\n",
    "\n",
    "articles = pd.read_csv(dataset_articles)\n",
    "\n",
    "transactions = pd.read_csv(dataset_transactions)\n",
    "\n",
    "print(\"loading finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CPT\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('C:/Users/Alessia/Documents/CPT-master/CPT-master/modules'))\n",
    "import CPT\n",
    "import PredictionTree\n",
    "from CPT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CPT model\n",
    "# model = CPT()\n",
    "# data,target = model.load_files(\"C:/Users/Alessia/Documents/GitHub/H-M-Sales-CRISP-DM-Analysis/trainprova.csv\",\"C:/Users/Alessia/Documents/GitHub/H-M-Sales-CRISP-DM-Analysis/testprova.csv\")\n",
    "# print(data)\n",
    "# model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using CPT, using the last 5 data in each sequence, and I should predict the next 3 items\n",
    "\n",
    "# predictions = model.predict(data,target,5,3)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371980 7\n",
      "105542 25\n",
      "31788324 5\n"
     ]
    }
   ],
   "source": [
    "#### EXPLORATORY ANALYSIS\n",
    "\n",
    "# Number of features and rows in Customers \n",
    "c_rows = customers.shape[0]\n",
    "c_features = customers.shape[1]\n",
    "print (c_rows, c_features)\n",
    "\n",
    "# Number of features and rows in Articles\n",
    "a_rows = articles.shape[0]\n",
    "a_features = articles.shape[1]\n",
    "print (a_rows, a_features)\n",
    "\n",
    "# Number of features and rows in Transactions\n",
    "t_rows = transactions.shape[0]\n",
    "t_features = transactions.shape[1] \n",
    "print(t_rows, t_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-20 2020-09-22\n"
     ]
    }
   ],
   "source": [
    "# Getting knowledge about updatedness of data\n",
    "print(transactions.iloc[1]['t_dat'], transactions.iloc[-1]['t_dat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers features: 7\n",
      "customer_id                object\n",
      "FN                        float64\n",
      "Active                    float64\n",
      "club_member_status         object\n",
      "fashion_news_frequency     object\n",
      "age                       float64\n",
      "postal_code                object\n",
      "dtype: object\n",
      "###########################\n",
      "Articles features: 25\n",
      "article_id                       int64\n",
      "product_code                     int64\n",
      "prod_name                       object\n",
      "product_type_no                  int64\n",
      "product_type_name               object\n",
      "product_group_name              object\n",
      "graphical_appearance_no          int64\n",
      "graphical_appearance_name       object\n",
      "colour_group_code                int64\n",
      "colour_group_name               object\n",
      "perceived_colour_value_id        int64\n",
      "perceived_colour_value_name     object\n",
      "perceived_colour_master_id       int64\n",
      "perceived_colour_master_name    object\n",
      "department_no                    int64\n",
      "department_name                 object\n",
      "index_code                      object\n",
      "index_name                      object\n",
      "index_group_no                   int64\n",
      "index_group_name                object\n",
      "section_no                       int64\n",
      "section_name                    object\n",
      "garment_group_no                 int64\n",
      "garment_group_name              object\n",
      "detail_desc                     object\n",
      "dtype: object\n",
      "###########################\n",
      "Transactions features: 5\n",
      "t_dat                object\n",
      "customer_id          object\n",
      "article_id            int64\n",
      "price               float64\n",
      "sales_channel_id      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Exploring the features of each dataset\n",
    "print(\"Customers features: \" + str(c_features))\n",
    "print(customers.dtypes)\n",
    "print(\"###########################\")\n",
    "print(\"Articles features: \" + str(a_features))\n",
    "print(articles.dtypes)\n",
    "print(\"###########################\")\n",
    "print(\"Transactions features: \" + str(t_features))\n",
    "print(transactions.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352899\n"
     ]
    }
   ],
   "source": [
    "# How many different customers postal codes do we have?\n",
    "print(len(customers['postal_code'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id                    0\n",
      "FN                        895050\n",
      "Active                    907576\n",
      "club_member_status          6062\n",
      "fashion_news_frequency     16009\n",
      "age                        15861\n",
      "postal_code                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are customer rows where some features are null\n",
    "print(customers.isna().sum())\n",
    "\n",
    "# Drop all columns where age is NaN\n",
    "customers = customers[customers[\"age\"].notna()]\n",
    "\n",
    "# Fill the NaN values in Active\n",
    "customers['Active'] = customers['Active'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the occurrences of NONE with None\n",
    "\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].replace(['NONE'],'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns where \"club_member_status\" and \"fashion_news_frequency\" are NaN\n",
    "customers = customers[customers[\"club_member_status\"].notna()]\n",
    "customers = customers[customers[\"fashion_news_frequency\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract non null values from \"club_member_status\" column and use them to fill the NaN values of the same column\n",
    "\n",
    "# values = customers['club_member_status'].value_counts(dropna=True)\n",
    "\n",
    "# a_v = customers[customers['club_member_status'].notna()]\n",
    "# active_values=a_v['club_member_status'].unique()\n",
    "# customers['club_member_status'] = customers['club_member_status'].fillna(np.random.choice(active_values.tolist()))\n",
    "# print(customers['club_member_status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Che cosa sto facendo sostanzialmente?\n",
    "Ho visto che per gli attributi \"fashion_news_frequency\" e \"club_member_status\" i valori nulli erano pochi rispetto alle totali istanze del dataset, per questo decido di dropparle dal dataset. Per quanto riguarda invece l'attributo FN, le istanze con i valori nulli sono molte rispetto al totale, quindi non posso rimuovere tutte quelle che non possiedono un valore per FN. Per questo motivo decido di non prendere in considerazione questa feature per la mia analisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 0 \n",
    "customers['FN'] = customers['FN'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] ['ACTIVE' 'PRE-CREATE' 'LEFT CLUB'] ['None' 'Regularly' 'Monthly'] [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Check the possible values for each of the following attributes\n",
    "print(customers['Active'].unique(), customers['club_member_status'].unique(),customers['fashion_news_frequency'].unique(),customers['FN'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id               1338570\n",
      "FN                        1338570\n",
      "Active                    1338570\n",
      "club_member_status        1338570\n",
      "fashion_news_frequency    1338570\n",
      "age                       1338570\n",
      "postal_code               1338570\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# At this point i have 1338570 homogeneous records for Customers\n",
    "print(customers.notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_dat               0\n",
      "customer_id         0\n",
      "article_id          0\n",
      "price               0\n",
      "sales_channel_id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the transactions\n",
    "print(transactions.isna().sum())\n",
    "\n",
    "# The records for the transactions are all consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I split the customers\n",
    "customers_1 = customers[customers['customer_id'] < '669286']\n",
    "customers_2 = customers[customers['customer_id'] >= '669286']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging customers and transactions\n",
    "customers_transactions_1 = pd.merge(customers_1, transactions, on='customer_id')\n",
    "customers_transactions_2 = pd.merge(customers_2, transactions, on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_transactions = customers_transactions_1.append(customers_transactions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2fe6fc2589ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Double check of the number of customers without any transaction. There are 9583 customers without any transaction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mspecial_customers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustomers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcustomers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"customer_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomers_transactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"customer_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecial_customers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[0mCategories\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m'b'\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1263\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Print unique values of customer_id in the whole dataframe customers_transactions\n",
    "print(len(customers_transactions[\"customer_id\"].unique()))\n",
    "\n",
    "# Double check of the number of customers without any transaction. There are 9583 customers without any transaction\n",
    "special_customers = customers[~customers[\"customer_id\"].astype(str).isin(customers_transactions[\"customer_id\"].astype(str).unique())]\n",
    "print(len(special_customers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split transactions dataframe because it's too large\n",
    "#trans_2018 = transactions[transactions['t_dat'] < \"2019-01-01\"]\n",
    "#trans_2019 = transactions[(transactions['t_dat'] < \"2020-01-01\") & (transactions['t_dat'] >= \"2019-01-01\")]\n",
    "#trans_2020 = transactions[transactions['t_dat'] >= \"2020-01-01\"]\n",
    "\n",
    "#print(len(trans_2018),len(trans_2019),len(trans_2020))\n",
    "\n",
    "# Check if the split was successful\n",
    "#print(trans_2018.iloc[1]['t_dat'], trans_2018.iloc[-1]['t_dat'])\n",
    "#print(trans_2019.iloc[1]['t_dat'], trans_2019.iloc[-1]['t_dat'])\n",
    "#print(trans_2020.iloc[1]['t_dat'], trans_2020.iloc[-1]['t_dat'])\n",
    "\n",
    "# Merge dataframes Customers and Transactions\n",
    "# customers_transactions_2018 = pd.merge(customers, trans_2018, on='customer_id')\n",
    "\n",
    "# customers_transactions_2019 = pd.merge(customers_transactions_2018, trans_2019, on='customer_id')\n",
    "# print(customers_transactions_2019.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_transactions[\"sales_channel_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of articles purchased by each Customer in 2018\n",
    "df_2018 = customers_transactions.query('t_dat < \"2019-01-01\"')\n",
    "\n",
    "# Filter out labels of interest\n",
    "df_2018 = df_2018.filter(['customer_id', 'article_id'])\n",
    "df_2018.rename(columns = {'article_id':'tot_articles_2018'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of articles purchased by each Customer in 2019\n",
    "df_2019 = customers_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    " \n",
    "# Filter out labels of interest\n",
    "df_2019 = df_2019.filter(['customer_id', 'article_id'])\n",
    "df_2019.rename(columns = {'article_id':'tot_articles_2019'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of articles purchased by each Customer in 2020\n",
    "df_2020 = customers_transactions.query('t_dat >= \"2020-01-01\"')\n",
    " \n",
    "# Filter out labels of interest\n",
    "df_2020 = df_2020.filter(['customer_id', 'article_id'])\n",
    "df_2020.rename(columns = {'article_id':'tot_articles_2020'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new features into Customers dataset\n",
    "customers = pd.merge(customers,df_2018.groupby('customer_id').count(), on='customer_id')\n",
    "customers = pd.merge(customers,df_2019.groupby('customer_id').count(), on='customer_id')\n",
    "customers = pd.merge(customers,df_2020.groupby('customer_id').count(), on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW ######\n",
    "# Merge with unique articles\n",
    "\n",
    "\"\"\"\n",
    "print(customers.head())\n",
    "\n",
    "df_2018_unique = df_2018.filter(['customer_id', 'tot_articles_2018'])\n",
    "df_2018_unique = df_2018_unique.groupby('customer_id').apply(lambda x: x.tot_articles_2018.nunique())\n",
    "\n",
    "df_2019_unique = df_2019.filter(['customer_id', 'tot_articles_2019'])\n",
    "df_2019_unique = df_2019_unique.groupby('customer_id').apply(lambda x: x.tot_articles_2019.nunique())\n",
    "\n",
    "df_2020_unique = df_2020.filter(['customer_id', 'tot_articles_2020'])\n",
    "df_2020_unique = df_2020_unique.groupby('customer_id').apply(lambda x: x.tot_articles_2020.nunique())    \n",
    "\n",
    "\n",
    "customers = pd.merge(customers,df_2018_unique, on='customer_id', how='left')\n",
    "customers = pd.merge(customers,df_2019_unique, on='customer_id', how='left')\n",
    "customers = pd.merge(customers,df_2020_unique, on='customer_id',how='left') \n",
    "\n",
    "\n",
    "#customers.rename(columns = {'tot_articles_2018_x':'unique_articles_2018'}, inplace = True)\n",
    "#customers.rename(columns = {'tot_articles_2019_x':'unique_articles_2019'}, inplace = True)\n",
    "#customers.rename(columns = {'tot_articles_2020_x':'unique_articles_2020'}, inplace = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge average articles 2018,2019,2020\n",
    "customers = pd.merge(customers,df_2018.groupby('customer_id').count()/3, on='customer_id')\n",
    "customers = pd.merge(customers,df_2019.groupby('customer_id').count()/12, on='customer_id')\n",
    "customers = pd.merge(customers,df_2020.groupby('customer_id').count()/9, on='customer_id')\n",
    "\n",
    "# rename\n",
    "\n",
    "# Rename\n",
    "customers.rename(columns = {'tot_articles_2018_x':'tot_articles_2018'}, inplace = True)\n",
    "customers.rename(columns = {'tot_articles_2019_x':'tot_articles_2019'}, inplace = True)\n",
    "customers.rename(columns = {'tot_articles_2020_x':'tot_articles_2020'}, inplace = True)\n",
    "customers.rename(columns = {'tot_articles_2018_y':'avg_articles_2018'}, inplace = True)\n",
    "customers.rename(columns = {'tot_articles_2019_y':'avg_articles_2019'}, inplace = True)\n",
    "customers.rename(columns = {'tot_articles_2020_y':'avg_articles_2020'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.head())\n",
    "print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of transactions of type 1 for each customer\n",
    "trans_type1 = customers_transactions.query('sales_channel_id == 1')\n",
    "trans_type1 = trans_type1.filter(['customer_id', 'sales_channel_id'])\n",
    "trans_type1.rename(columns = {'sales_channel_id':'sales_channel_1'}, inplace = True)\n",
    "print(trans_type1.groupby('customer_id').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of transactions of type 2 for each customer\n",
    "trans_type2 = customers_transactions.query('sales_channel_id == 2')\n",
    "trans_type2 = trans_type2.filter(['customer_id', 'sales_channel_id'])\n",
    "trans_type2.rename(columns = {'sales_channel_id':'sales_channel_2'}, inplace = True)\n",
    "print(trans_type2.groupby('customer_id').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new features into customers table\n",
    "\n",
    "customers = pd.merge(customers, trans_type1.groupby('customer_id').count(), on='customer_id')\n",
    "print(customers.shape)\n",
    "customers = pd.merge(customers,trans_type2.groupby('customer_id').count(), on='customer_id')\n",
    "print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Customers with Articles \n",
    "# customers_articles = pd.merge(pd.read_csv(dataset_customers), pd.read_csv(dataset_articles), on ='customer_id')\n",
    "print(customers_transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check some variables about the price column\n",
    "\n",
    "# Mean of the data\n",
    "\n",
    "# mean = statistics.fmean(transactions[\"price\"])\n",
    "# print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square deviations\n",
    "\n",
    "# stdev = statistics.stdev(transactions[\"price\"])\n",
    "# print(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "# variance = statistics.variance(transactions[\"price\"])\n",
    "# print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm\n",
    "# norm = ln.norm(transactions[\"price\"])\n",
    "#print(norm)\n",
    "\n",
    "# Mean 0.027829273856993762\n",
    "# Std-deviation 0.019181128054793647\n",
    "# Variance 0.00036791567345439193\n",
    "# Norm 190.56357821702613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have checked that my values are standardized, but the values are not satisfactory.\n",
    "# So I decide to scale my values multiplying them for 10^3 in order to make them prices that are more realistic.\n",
    "# I have that my lowest value is 0.01 and my biggest is 591.52\n",
    "#transactions[\"price\"] = transactions[\"price\"] * (pow(10,3)) \n",
    "\n",
    "\n",
    "# Print the new min and max values, and the rows where they appear.\n",
    "print(transactions[\"price\"].max(), transactions[\"price\"].idxmax(),  transactions[\"price\"].min(),transactions[\"price\"].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pick the important columns from transactions and then merge them with customers table\n",
    "am_2018 = transactions.query('t_dat < \"2019-01-01\"')\n",
    "\n",
    "#Filter out labels of interest\n",
    "am_2018 = am_2018.filter(['customer_id', 'price'])\n",
    "am_2018.rename(columns = {'price':'tot_amount_2018'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pick the important columns from transactions and then merge them with customers table\n",
    "am_2019 = transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "\n",
    "#Filter out labels of interest\n",
    "am_2019 = am_2019.filter(['customer_id', 'price'])\n",
    "am_2019.rename(columns = {'price':'tot_amount_2019'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pick the important columns from transactions and then merge them with customers table\n",
    "am_2020 = transactions.query('t_dat >= \"2020-01-01\"')\n",
    "\n",
    "#Filter out labels of interest\n",
    "am_2020 = am_2020.filter(['customer_id', 'price'])\n",
    "am_2020.rename(columns = {'price':'tot_amount_2020'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new tot_amount features into customers table\n",
    "customers = pd.merge(customers, am_2018.groupby('customer_id').sum(), on='customer_id')\n",
    "customers = pd.merge(customers, am_2019.groupby('customer_id').sum(), on='customer_id')\n",
    "customers = pd.merge(customers, am_2020.groupby('customer_id').sum(), on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new avg amounts features into customers table\n",
    "customers = pd.merge(customers, am_2018.groupby('customer_id').sum()/3, on='customer_id')\n",
    "customers = pd.merge(customers, am_2019.groupby('customer_id').sum()/12, on='customer_id')\n",
    "customers = pd.merge(customers, am_2020.groupby('customer_id').sum()/9, on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in customers\n",
    "customers.rename(columns = {'tot_amount_2018_y':'avg_amount_2018'}, inplace = True)\n",
    "customers.rename(columns = {'tot_amount_2019_y':'avg_amount_2019'}, inplace = True)\n",
    "customers.rename(columns = {'tot_amount_2020_y':'avg_amount_2020'}, inplace = True)\n",
    "customers.rename(columns = {'tot_amount_2018_x':'tot_amount_2018'}, inplace = True)\n",
    "customers.rename(columns = {'tot_amount_2019_x':'tot_amount_2019'}, inplace = True)\n",
    "customers.rename(columns = {'tot_amount_2020_x':'tot_amount_2020'}, inplace = True)\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer sex of the customers thanks to the purchased articles\n",
    "\n",
    "# Analyze possible categories\n",
    "print(articles[\"index_group_name\"].unique())\n",
    "print(articles[\"index_group_no\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find how many articles we have divided for categories\n",
    "sex_category = articles[[\"index_group_no\", \"index_group_name\"]].reset_index()\n",
    "display(sex_category[\"index_group_name\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the list of categories of articles and we plot the percentages of articles categories\n",
    "sex_category_list = sex_category[\"index_group_name\"].value_counts().index.to_list()\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.pie(sex_category[\"index_group_name\"].value_counts().sort_values(ascending=False), \n",
    "        labels = sex_category_list, startangle = 90, counterclock=False, autopct=\"%1.1f%%\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I rename the index group as sex attribute\n",
    "articles_category_df = pd.DataFrame(articles[[\"article_id\", \"index_group_no\"]])\n",
    "articles_category_df.columns = [\"article_id\", \"sex_attribute\"]\n",
    "articles_category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I merge these sex attributes with the dataframe customers_transactions\n",
    "customers_transactions = pd.merge(customers_transactions, articles_category_df, on = \"article_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sex = customers_transactions[[\"customer_id\", \"sex_attribute\", \"article_id\"]].groupby([\"customer_id\",\"sex_attribute\"]).count().unstack()\n",
    "cust_sex.columns = [\"Woman\", \"Young\", \"Man\", \"Have-kids\", \"Sports-person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values\n",
    "cust_sex = cust_sex.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these attributes into the customers dataset \n",
    "customers = pd.merge(customers, cust_sex, on ='customer_id')\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis on the age ranges\n",
    "print(customers[\"age\"].min(),customers[\"age\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON FUNZIONA\n",
    "\n",
    "#customers.loc[customers['age'] >= 16 & customers['age'] < 36,customers['age']] = \"young adult\"\n",
    "#customers['age'] = customers['age'].astype(str)\n",
    "#print(customers.dtypes)\n",
    "#customers['age'] = customers['age'].mask(customers['age'] >= 66, \"senior\", inplace=True)\n",
    "#customers['age'] = customers['age'].mask((customers['age'] >= 16) & (customers['age'] < 36), \"young adult\", inplace=True)\n",
    "#customers['age'] = customers['age'].mask((customers['age'] >= 36) & (customers['age'] < 51), \"adult\", inplace=True)\n",
    "#customers['age'] = customers['age'].mask((customers['age'] >= 51) & (customers['age'] < 66), \"mid age adult\", inplace=True)\n",
    "#print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace continuous ages with discrete ages groups\n",
    "#ages_group = customers.query('age >= 16 & age < 36')\n",
    "#ages_group = ages_group.filter(['customer_id', 'age'])\n",
    "#ages_group.loc[(ages_group['age']>= 16) & (ages_group['age'] < 36) , 'age'] = 'young adult'\n",
    "\n",
    "#ages_group = customers.query('age >= 36 & age < 51')\n",
    "#ages_group = ages_group.filter(['customer_id', 'age'])\n",
    "#ages_group.loc[(ages_group['age']>= 36) & (ages_group['age'] < 51) , 'age'] = 'adult'\n",
    "\n",
    "#ages_group = customers.query('age >= 51 & age < 66')\n",
    "#ages_group = ages_group.filter(['customer_id', 'age'])\n",
    "#ages_group.loc[(ages_group['age']>= 51) & (ages_group['age'] < 66) , 'age'] = 'mid aged adult'\n",
    "\n",
    "#ages_group = customers.query('age >= 65')\n",
    "#ages_group = ages_group.filter(['customer_id', 'age'])\n",
    "#ages_group.loc[ages_group['age']>= 65 , 'age'] = 'senior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ages before the transformation\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign a numeric category for each age range\n",
    "\n",
    "#customers.loc[(customers['age']>= 16) & (customers['age']< 36) , 'age'] = 1\n",
    "#customers.loc[(customers['age'] >= 36) & (customers['age'] < 51), 'age'] = 2\n",
    "#customers.loc[(customers['age'] >= 51) & (customers['age'] < 66) , 'age'] = 3\n",
    "#customers.loc[customers['age']>= 66, 'age'] = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numeric categories\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I replace numeric categories with categorical attributes\n",
    "#customers['age'].replace({'young adult':1 },inplace=True)\n",
    "#customers['age'].replace({'adult': 2},inplace=True)\n",
    "#customers['age'].replace({'mid aged adult': 3},inplace=True)\n",
    "#customers['age'].replace({'senior':4 },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop postal code \n",
    "\n",
    "# I drop postal code because it doesn't tell us anything about the behaviour of the customer in the field of shopping. It's just a string\n",
    "customers= customers.drop('postal_code',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find the garment and the color more purchased for each year\n",
    "transac_drop_df = pd.DataFrame(transactions[['customer_id', 'article_id', 't_dat']]).reset_index()\n",
    "articles_drop_df = pd.DataFrame(articles[['article_id', 'colour_group_code', 'garment_group_no',\"perceived_colour_master_id\", 'section_no']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_transactions =  pd.merge(transac_drop_df, articles_drop_df, on='article_id', how=\"left\")\n",
    "print(articles_transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of articles for each customer that are purchased more than one time\n",
    "\"\"\"customers['count_purchased_more_than_one']=''\n",
    "#df=customers_transactions.filter(['customer_id', 'article_id'])\n",
    "df = customers_transactions[customers_transactions.groupby(['customer_id','article_id'])['article_id'].transform('size').lt(2)]\n",
    "\n",
    "# unique articles 23546342\n",
    "# 7925230\n",
    "\n",
    "## Articles per customer bought only one time \n",
    "df['new_col']=''\n",
    "df['new_col'] = df.groupby([\"customer_id\",'article_id']).count()\n",
    "print(df)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find the counts for each colour\n",
    "## POSSIBLE COLOURS ARE 50, POSSIBLE GARMENTS ARE 21\n",
    "customer_articles_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "customer_articles_2018 = customer_articles_2018.filter(['customer_id', 'colour_group_code'])\n",
    "customers = pd.merge(customers,customer_articles_2018.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'colour_group_code':'count_colours_2018'}, inplace = True)\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_articles_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "customer_articles_2019 = customer_articles_2019.filter(['customer_id', 'colour_group_code'])\n",
    "customers = pd.merge(customers,customer_articles_2019.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'colour_group_code':'count_colours_2019'}, inplace = True)\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_articles_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "customer_articles_2020 = customer_articles_2020.filter(['customer_id', 'colour_group_code'])\n",
    "customers = pd.merge(customers,customer_articles_2020.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'colour_group_code':'count_colours_2020'}, inplace = True)\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_garments_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "customer_garments_2018 = customer_garments_2018.filter(['customer_id', 'garment_group_no'])\n",
    "customers = pd.merge(customers,customer_garments_2018.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'garment_group_no':'count_garments_2018'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_garments_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "customer_garments_2019 = customer_garments_2019.filter(['customer_id', 'garment_group_no'])\n",
    "customers = pd.merge(customers,customer_garments_2019.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'garment_group_no':'count_garments_2019'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_garments_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "customer_garments_2020 = customer_garments_2020.filter(['customer_id', 'garment_group_no'])\n",
    "customers = pd.merge(customers,customer_garments_2020.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'garment_group_no':'count_garments_2020'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_articles_2018 = customers_articles_2018.groupby(['customer_id'])['colour_group_code'].agg(pd.Series.mode).to_frame()\n",
    "\n",
    "# Find the mode of the most purchased color for 2018\n",
    "#customers_articles_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "#customers_articles_2018 = customers_articles_2018.groupby(['customer_id'])['colour_group_code'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_articles_2018.rename(columns = {'colour_group_code':'top_colour_2018'}, inplace = True)\n",
    "#print(customers_articles_2018.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most purchased color for 2019\n",
    "#customers_articles_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "#customers_articles_2019 = customers_articles_2019.groupby(['customer_id'])['colour_group_code'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_articles_2019.rename(columns = {'colour_group_code':'top_colour_2019'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most purchased color for 2020\n",
    "#customers_articles_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "#customers_articles_2020 = customers_articles_2020.groupby(['customer_id'])['colour_group_code'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_articles_2020.rename(columns = {'colour_group_code':'top_colour_2020'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new df in the customers table\n",
    "#customers = pd.merge(customers, customers_articles_2018, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_articles_2019, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_articles_2020, on ='customer_id',how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(customers.head())\n",
    "#print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(customers['top_colour_2019'].unique())\n",
    "\n",
    "#print(articles[articles['colour_group_code']== -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most purchased garment for 2018\n",
    "#customers_garments_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "#customers_garments_2018 = customers_garments_2018.groupby(['customer_id'])['garment_group_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_garments_2018.rename(columns = {'garment_group_no':'top_garment_2018'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most purchased garment for 2019\n",
    "#customers_garment_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "#customers_garment_2019 = customers_garment_2019.groupby(['customer_id'])['garment_group_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_garment_2019.rename(columns = {'garment_group_no':'top_garment_2019'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most purchased garment for 2020\n",
    "#customers_garment_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "#customers_garment_2020 = customers_garment_2020.groupby(['customer_id'])['garment_group_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_garment_2020.rename(columns = {'garment_group_no':'top_garment_2020'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these new df in the customers table\n",
    "#customers = pd.merge(customers, customers_garments_2018, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_garment_2019, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_garment_2020, on ='customer_id',how=\"left\")\n",
    "#print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most visited section for 2018\n",
    "#customers_section_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "#customers_section_2018 = customers_section_2018.groupby(['customer_id'])['section_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_section_2018.rename(columns = {'section_no':'top_section_2018'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most visited section for 2019\n",
    "#customers_section_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "#customers_section_2019 = customers_section_2019.groupby(['customer_id'])['section_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_section_2019.rename(columns = {'section_no':'top_section_2019'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mode of the most visited section for 2020\n",
    "#customers_section_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "#customers_section_2020 = customers_section_2020.groupby(['customer_id'])['section_no'].apply(lambda x: x.mode().iloc[0]).to_frame()\n",
    "#customers_section_2020.rename(columns = {'section_no':'top_section_2020'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers = pd.merge(customers, customers_section_2018, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_section_2019, on ='customer_id',how=\"left\")\n",
    "#customers = pd.merge(customers, customers_section_2020, on ='customer_id',how=\"left\")\n",
    "#print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers.to_csv(\"customers_complete_numeric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace names\n",
    "# I replace numeric categories with categorical attributes\n",
    "#customers['club_member_status'].replace({'ACTIVE':1 },inplace=True)\n",
    "#customers['club_member_status'].replace({'PRE-CREATE':2},inplace=True)\n",
    "#customers['club_member_status'].replace({'LEFT CLUB':0},inplace=True)\n",
    "\n",
    "#customers['fashion_news_frequency'].replace({'None': 0},inplace=True)\n",
    "#customers['fashion_news_frequency'].replace({'Regularly': 1},inplace=True)\n",
    "#customers['fashion_news_frequency'].replace({'Monthly':2 },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers.to_csv(\"customers_complete_numeric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers['Active'].unique(), customers['FN'].unique(), customers['club_member_status'].unique(),customers['fashion_news_frequency'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would drop FN because only 84 customers (0.03%) that have fashion_news_frequency==None have FN==1, but I don't\n",
    "#rslt_df = customers['fashion_news_frequency'][customers['FN'] == 1]\n",
    "#print(rslt_df[rslt_df == 'None'].count())\n",
    "#print(customers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to create a new feature, instead of count colors i try to make count perceived/ count colours\n",
    "customers_perceived_2018 = articles_transactions.query('t_dat < \"2019-01-01\"')\n",
    "customers_perceived_2018 = customers_perceived_2018.filter(['customer_id', 'perceived_colour_master_id'])\n",
    "customers = pd.merge(customers,customers_perceived_2018.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'perceived_colour_master_id':'count_perceived_2018'}, inplace = True)\n",
    "\n",
    "customers_perceived_2019 = articles_transactions.query('t_dat < \"2020-01-01\" & t_dat>=\"2019-01-01\"')\n",
    "customers_perceived_2019 = customers_perceived_2019.filter(['customer_id', 'perceived_colour_master_id'])\n",
    "customers = pd.merge(customers,customers_perceived_2019.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'perceived_colour_master_id':'count_perceived_2019'}, inplace = True)\n",
    "\n",
    "customers_perceived_2020 = articles_transactions.query('t_dat >= \"2020-01-01\"')\n",
    "customers_perceived_2020 = customers_perceived_2020.filter(['customer_id', 'perceived_colour_master_id'])\n",
    "customers = pd.merge(customers,customers_perceived_2020.groupby('customer_id').nunique(), on='customer_id')\n",
    "customers.rename(columns = {'perceived_colour_master_id':'count_perceived_2020'}, inplace = True)\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column related to the ratio of number of perceived colours/number of colours\n",
    "print(customers.keys())\n",
    "customers['ratio_perc_concrete_2018']= \"\"\n",
    "customers['ratio_perc_concrete_2018'] = customers['count_perceived_2018']/customers['count_colours_2018'] \n",
    "print(customers.head())\n",
    "\n",
    "customers['ratio_perc_concrete_2019']= \"\"\n",
    "customers['ratio_perc_concrete_2019'] = customers['count_perceived_2019']/customers['count_colours_2019'] \n",
    "print(customers.head())\n",
    "\n",
    "customers['ratio_perc_concrete_2020']= \"\"\n",
    "customers['ratio_perc_concrete_2020'] = customers['count_perceived_2020']/customers['count_colours_2020'] \n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop count perceived because they were helpful only to calculate the ratio\n",
    "#customers = customers.drop(\"count_perceived_2018\", 1)\n",
    "#customers = customers.drop(\"count_perceived_2019\", 1)\n",
    "#customers = customers.drop(\"count_perceived_2020\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I CREATE THE FEATURE COUNT GARMENTS/TOTAL GARMENTS INSTEAD OF COUNT GARMENTS ONLY (to remove from here)\n",
    "tot_garments= len(articles['garment_group_no'].unique())\n",
    "customers['ratio_garment_total_2018']=\"\"\n",
    "customers['ratio_garment_total_2019']=\"\"\n",
    "customers['ratio_garment_total_2020']=\"\"\n",
    "customers['ratio_garment_total_2018']= customers['count_garments_2018']/customers['tot_articles_2018']\n",
    "customers['ratio_garment_total_2019']= customers['count_garments_2019']/customers['tot_articles_2019']\n",
    "customers['ratio_garment_total_2020']= customers['count_garments_2020']/customers['tot_articles_2020']\n",
    "\n",
    "\n",
    "#customers = customers.drop(\"count_garments_2018_zscore\", 1)\n",
    "#customers = customers.drop(\"count_garments_2019_zscore\", 1)\n",
    "#customers = customers.drop(\"count_garments_2020_zscore\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I average the avg articles and the avg amount\n",
    "customers['avg_amount']=\"\"\n",
    "customers['avg_articles']=\"\"\n",
    "customers['avg_amount'] = customers['avg_amount_2018']+customers['avg_amount_2019']+customers['avg_amount_2020']/3\n",
    "customers['avg_articles'] = customers['avg_articles_2018']+customers['avg_articles_2019']+customers['avg_articles_2020']/3\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform z-score normalization on variables that are not one hot encoded\n",
    "cols = customers.columns\n",
    "cols = cols.drop(\"customer_id\", 1)\n",
    "cols = cols.drop(\"fashion_news_frequency\", 1)\n",
    "cols = cols.drop(\"FN\", 1)\n",
    "cols = cols.drop(\"club_member_status\", 1)\n",
    "cols = cols.drop(\"Active\", 1)\n",
    "cols=list(cols)\n",
    "print(cols)\n",
    "\n",
    "customers_norm=pd.DataFrame(customers)   \n",
    "# now iterate over the remaining columns and create a new zscore column\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    customers_norm[col_zscore] = (customers_norm[col] - customers_norm[col].mean())/customers_norm[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not normalized\n",
    "for col in cols:\n",
    "    if '_zscore' not in col:\n",
    "        customers_norm = customers_norm.drop(col, 1)\n",
    "    else:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of Active variable\n",
    "\n",
    "df = pd.DataFrame(list(zip(customers_norm['customer_id'], customers_norm['Active'].unique())),\n",
    "                  columns=['customer_id', 'Active'])\n",
    "\n",
    "y = pd.get_dummies(customers_norm['Active'], prefix='')\n",
    "y.rename(columns = {'_0.0':'Non-Active'}, inplace = True)\n",
    "y.rename(columns = {'_1.0':'Active'}, inplace = True)\n",
    "y = y.astype({\"Active\": \"float64\"}, errors='raise') \n",
    "y = y.astype({\"Non-Active\": \"float64\"}, errors='raise') \n",
    "y.insert(0, 'customer_id', customers['customer_id'])\n",
    "\n",
    "customers_norm = customers_norm.drop(\"Active\", 1)\n",
    "customers_norm = pd.merge(customers_norm, y, on='customer_id', how ='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of FN variable\n",
    "\n",
    "df = pd.DataFrame(list(zip(customers_norm['customer_id'], customers_norm['FN'].unique())),\n",
    "                  columns=['customer_id', 'FN'])\n",
    "\n",
    "y = pd.get_dummies(customers_norm['FN'], prefix='')\n",
    "y.rename(columns = {'_0.0':'FN_no'}, inplace = True)\n",
    "y.rename(columns = {'_1.0':'FN_yes'}, inplace = True)\n",
    "y = y.astype({\"FN_yes\": \"float64\"}, errors='raise') \n",
    "y = y.astype({\"FN_no\": \"float64\"}, errors='raise') \n",
    "y.insert(0, 'customer_id', customers_norm['customer_id'])\n",
    "\n",
    "customers_norm = customers_norm.drop(\"FN\", 1)\n",
    "customers_norm = pd.merge(customers_norm, y, on='customer_id', how ='left')\n",
    "print(customers_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of club_member_status variable\n",
    "\n",
    "df = pd.DataFrame(list(zip(customers_norm['customer_id'], customers_norm['club_member_status'].unique())),\n",
    "                  columns=['customer_id', 'club_member_status'])\n",
    "\n",
    "y = pd.get_dummies(customers_norm['club_member_status'], prefix='')\n",
    "y.rename(columns = {'_ACTIVE':'club_status_active'}, inplace = True)\n",
    "y.rename(columns = {'_PRE-CREATE':'club_status_precreate'}, inplace = True)\n",
    "y.rename(columns = {'_LEFT CLUB':'club_status_leftclub'}, inplace = True)\n",
    "y = y.astype({\"club_status_active\": \"float64\"}, errors='raise') \n",
    "y = y.astype({\"club_status_precreate\": \"float64\"}, errors='raise')\n",
    "y = y.astype({\"club_status_leftclub\": \"float64\"}, errors='raise') \n",
    "y.insert(0, 'customer_id', customers_norm['customer_id'])\n",
    "\n",
    "customers_norm = customers_norm.drop(\"club_member_status\", 1)\n",
    "customers_norm = pd.merge(customers_norm, y, on='customer_id', how ='left')\n",
    "print(customers_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of fashion_news_frequency variable\n",
    "\n",
    "df = pd.DataFrame(list(zip(customers_norm['customer_id'], customers_norm['fashion_news_frequency'].unique())),\n",
    "                  columns=['customer_id', 'fashion_news_frequency'])\n",
    "\n",
    "y = pd.get_dummies(customers_norm['fashion_news_frequency'], prefix='')\n",
    "y.rename(columns = {'_None':'fn_frequency_none'}, inplace = True)\n",
    "y.rename(columns = {'_Regularly':'fn_frequency_regularly'}, inplace = True)\n",
    "y.rename(columns = {'_Monthly':'fn_frequency_monthly'}, inplace = True)\n",
    "y = y.astype({\"fn_frequency_none\": \"float64\"}, errors='raise') \n",
    "y = y.astype({\"fn_frequency_regularly\": \"float64\"}, errors='raise') \n",
    "y = y.astype({\"fn_frequency_monthly\": \"float64\"}, errors='raise') \n",
    "y.insert(0, 'customer_id', customers['customer_id'])\n",
    "\n",
    "customers_norm = customers_norm.drop(\"fashion_news_frequency\", 1)\n",
    "customers_norm = pd.merge(customers_norm, y, on='customer_id', how ='left')\n",
    "print(customers_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_norm.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers.to_csv(\"customers_numeric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = statistics.fmean(customers_norm[\"age_zscore\"])\n",
    "print(mean)\n",
    "stdev = statistics.stdev(customers_norm[\"age_zscore\"])\n",
    "print(stdev)\n",
    "#variance = statistics.variance(transactions[\"price\"])\n",
    "#print(variance)\n",
    "#norm = ln.norm(transactions[\"price\"])\n",
    "#print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers = customers.drop(\"customer_id\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_norm.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di correlazione\n",
    "corr_df = customers_norm.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_df.to_csv(\"correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AFTER THIS SHITTY ANALYSIS I DECIDE TO DROP ONE BETWEEN COUNT AND AVERAGE (FOR AMOUNT AND ARTICLES)\n",
    "\n",
    "customers_1 = customers_norm.drop(\"tot_articles_2018_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"tot_articles_2019_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"tot_articles_2020_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"tot_amount_2018_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"tot_amount_2019_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"tot_amount_2020_zscore\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER HAVING INTRODUCED THE RATIO, I REMOVE THE COUNT COLOURS\n",
    "customers_1 = customers_1.drop(\"count_colours_2018_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"count_colours_2019_zscore\", 1)\n",
    "customers_1 = customers_1.drop(\"count_colours_2020_zscore\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di correlazione\n",
    "corr_df = customers_1.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# I CREATE THE FEATURE COUNT GARMENTS/TOTAL GARMENTS INSTEAD OF COUNT GARMENTS ONLY\n",
    "tot_garments= len(articles['garment_group_no'].unique())\n",
    "print(tot_garments)\n",
    "customers['ratio_garment_total_2018']=\"\"\n",
    "customers['ratio_garment_total_2019']=\"\"\n",
    "customers['ratio_garment_total_2020']=\"\"\n",
    "customers['ratio_garment_total_2018']= customers['count_garments_2018_zscore']/tot_garments\n",
    "customers['ratio_garment_total_2019']= customers['count_garments_2019_zscore']/tot_garments\n",
    "customers['ratio_garment_total_2020']= customers['count_garments_2020_zscore']/tot_garments\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_2 = customers_1.drop(\"count_garments_2018_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"count_garments_2019_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"count_garments_2020_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"count_perceived_2018_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"count_perceived_2019_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"count_perceived_2020_zscore\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di correlazione\n",
    "corr_df = customers_2.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_2 = customers_2.drop(\"avg_articles_2018_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"avg_articles_2019_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"avg_articles_2020_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"avg_amount_2018_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"avg_amount_2019_zscore\", 1)\n",
    "customers_2 = customers_2.drop(\"avg_amount_2020_zscore\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di correlazione\n",
    "corr_df = customers_2.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(customers.filter(['sales_channel_2_zscore', 'Woman_zscore','Young_zscore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovering correlations...why sales_channel_2 is correlated \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "temp = transactions.groupby([\"sales_channel_id\"])[\"customer_id\"].count()\n",
    "print(temp.index, temp.values)\n",
    "df = pd.DataFrame({\"sales_channel\": temp.index, \"Customers\": temp.values})\n",
    "df = df.sort_values([\"sales_channel\"], ascending=False)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Number of customers for sales channel\")\n",
    "s = sns.barplot(x=\"sales_channel\", y=\"Customers\", data=df, palette=\"cubehelix\")\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=90)\n",
    "locs, labels = plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation between Woman count and transaction with sales_channel_2\n",
    "\n",
    "customers_norm.plot(kind = 'scatter', x = 'sales_channel_2_zscore', y = 'Woman_zscore')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Features are very correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation between sales_channel_2 and Young count\n",
    "\n",
    "customers_norm.plot(kind = 'scatter', x = 'sales_channel_2_zscore', y = 'Young_zscore')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation between avg_amount_zscore and avg_articles_zscore\n",
    "\n",
    "customers_norm.plot(kind = 'scatter', x = 'avg_amount_zscore', y = 'avg_articles_zscore')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I begin deleting sales_channel_2 and one hot encoded that are very correlated (Non-Active, FN_no)(fn_frequency_none, Non-Active)\n",
    "# (FN_yes, fn_frequency_regularly)\n",
    "customers_3 = customers_2.drop(\"sales_channel_2_zscore\", 1)\n",
    "customers_3 = customers_3.drop(\"FN_no\", 1)\n",
    "customers_3 = customers_3.drop(\"fn_frequency_none\", 1)\n",
    "customers_3 = customers_3.drop(\"FN_yes\", 1)\n",
    "customers_3 = customers_3.drop(\"Active\", 1)\n",
    "customers_3 = customers_3.drop(\"Non-Active\", 1)\n",
    "#customers=customers.drop('avg_amount_zscore',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di correlazione\n",
    "corr_df = customers_3.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_3=customers_3.drop('avg_articles_zscore',1)\n",
    "\n",
    "corr_df = customers_3.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_4=customers_3.drop('avg_amount_zscore', 1)\n",
    "\n",
    "corr_df = customers_4.corr(method='pearson')\n",
    "corr_df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this 18 features I apply clustering K Means with Elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(customers_4.dtypes)\n",
    "\n",
    "customers_5 = customers_4.drop ('customer_id', 1)\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(customers_5)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of clusters is 3, before the curve flattens\n",
    "kmeans_model = KMeans(n_clusters = 3)\n",
    "\n",
    "label = kmeans_model.fit_predict(customers_5.to_numpy())\n",
    "\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-dimensional plotting on two random dimensions\n",
    "'''u_labels = np.unique(label)\n",
    "\n",
    "print(u_labels)\n",
    " \n",
    "#plotting the results:\n",
    " \n",
    "for i in u_labels:\n",
    "    plt.scatter(customers_4.to_numpy()[label == i , 'club_status_active'] , customers_4.to_numpy()[label == i , 'age_zscore'] , label = i, alpha=0.5)\n",
    "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a multidimensional view on our data, so we plot parallel coodrinates\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "scaled_df = ss.fit_transform(customers_5.to_numpy())\n",
    "scaled_df = pd.DataFrame(scaled_df)\n",
    "customers_4[\"cluster\"] = kmeans_model.labels_\n",
    "final_df = pd.concat([scaled_df, customers_4['cluster']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "pc = parallel_coordinates(final_df, 'cluster', color=('#FFE888', '#FF9999','#3302ba'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cohordinates of the centroids\n",
    "\n",
    "print(kmeans_model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms that represent the distribution of the cluster in respect to each feature\n",
    "\n",
    "# How many instances do we have for each cluster?\n",
    "print(customers_4.head())\n",
    "temp = customers_4.groupby([\"cluster\"])[\"customer_id\"].count()\n",
    "print(temp.index, temp.values)\n",
    "df = pd.DataFrame({\"cluster\": temp.index, \"customers\": temp.values})\n",
    "df = df.sort_values([\"cluster\"], ascending=False)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Number of customers for sales channel\")\n",
    "s = sns.barplot(x=\"cluster\", y=\"customers\", data=df, palette=\"cubehelix\")\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation=90)\n",
    "locs, labels = plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since visualizing the clusters in this way is very low informative, i need to select the visualization of clusters among the\n",
    "# most promising features\n",
    "\n",
    "#filter rows of original data\n",
    "'''filtered_label0 = customers_4[label == 0]\n",
    " \n",
    "#plotting the results\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "customers_6=customers_4.drop('customer_id',1)\n",
    "\n",
    "silhouette_avg = silhouette_score(customers_6, customers_6['cluster'])\n",
    "print(silhouette_avg)\n",
    "\n",
    "# 0.1495051505822442 is the silhouette score without dimensionality reduction, with 3 clusters. Is near to 0, so clusters overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()\n",
    "\n",
    "cluster_df = pd.concat([\n",
    "   customers, \n",
    "   customers_6['cluster']], \n",
    "axis=1)\n",
    "\n",
    "for c in customers_6.columns[1:]:\n",
    "    grid = sns.FacetGrid(cluster_df, col='cluster', height=3, aspect=1.3)\n",
    "    grid.map(plt.hist, c, bins=20, edgecolor='k')\n",
    "    grid.set_xticklabels(rotation=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def prepare_pca(n_components, data, kmeans_labels):\n",
    "    names = ['x', 'y', 'z']\n",
    "    matrix = PCA(n_components=n_components).fit_transform(data)\n",
    "    df_matrix = pd.DataFrame(matrix)\n",
    "    df_matrix.rename({i:names[i] for i in range(n_components)}, axis=1, inplace=True)\n",
    "    df_matrix['labels'] = kmeans_labels\n",
    "    \n",
    "    return df_matrix'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Now I try performing the PCA and check if we can find more information about clusters that we have \n",
    "customers_5 = customers_4.drop('cluster',1)\n",
    "pca_df = prepare_pca(2, customers_5, kmeans_model.labels_)\n",
    "sns.scatterplot(x=pca_df.x, y=pca_df.y, hue=pca_df.labels, \n",
    "                palette=\"Set2\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pca = PCA(.95)\n",
    "principalComponents = pca.fit_transform(customers_5)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
